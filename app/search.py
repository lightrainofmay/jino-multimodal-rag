import openai
import numpy as np
import pandas as pd
import os
from dotenv import load_dotenv
from openai import OpenAI  # âœ… ä½¿ç”¨æ–°ç‰ˆ API å®¢æˆ·ç«¯

load_dotenv()

def extract_keywords(query, api_key):
    client = OpenAI(api_key=api_key)  # âœ… æ–°å†™æ³•

    prompt = f"""è¯·ä»ä¸‹é¢çš„ä¸­æ–‡é—®é¢˜ä¸­æå–æœ€æ ¸å¿ƒçš„æœç´¢å…³é”®è¯ï¼š

1. å¦‚æœé—®é¢˜æ˜¯ **"åŸºè¯ºè¯­çš„Xæ€ä¹ˆè¯´ï¼Ÿ"**ï¼Œä½ åº”è¯¥åªè¿”å› **"X"**ï¼Œä¸è¦è¿”å› "åŸºè¯ºè¯­"ã€‚
2. ä»…è¿”å›ä¸€ä¸ªæœ€ç›¸å…³çš„å…³é”®è¯ã€‚
3. ä¸è¦è¿”å›å¥å­æˆ–å¤šä½™çš„è§£é‡Šï¼Œåªè¿”å›å…³é”®è¯ã€‚

é—®é¢˜ï¼š{query}
å…³é”®è¯ï¼š"""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=5
    )
    keyword = response.choices[0].message.content.strip()
    print(f"ğŸ“ æå–å…³é”®è¯ï¼š{keyword}")
    return keyword


def semantic_search(query, df, index, model, top_k=5):
    print(f"ğŸ” æ­£åœ¨æœç´¢ï¼š{query}")
    query_embedding = model.encode(query, normalize_embeddings=True).reshape(1, -1)
    _, indices = index.search(query_embedding, top_k)
    valid_indices = [i for i in indices[0] if i < len(df)]
    return df.iloc[valid_indices]["text"].tolist()


def process_results(df, text_results):
    file_results = df[["text", "file"]].dropna().drop_duplicates()
    text_to_files = file_results.groupby("text")["file"].apply(list).to_dict()

    output = {}
    for text in text_results:
        files = text_to_files.get(text, [])
        output[text] = {
            "images": [f for f in files if f.endswith((".jpg", ".png", ".webp"))] or ["æš‚æ— å›¾ç‰‡"],
            "audios": [f for f in files if f.endswith((".mp3", ".wav", ".ogg"))] or ["æš‚æ— éŸ³é¢‘"],
        }
    return output


if __name__ == "__main__":
    import pickle
    import faiss
    from sentence_transformers import SentenceTransformer

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("âŒ æœªåœ¨ .env ä¸­æ‰¾åˆ° OPENAI_API_KEY")

    df = pd.read_json("data/jino_all_media.json", encoding="utf-8")
    index = faiss.read_index("data/faiss_index.bin")
    model = SentenceTransformer("moka-ai/m3e-base")

    query = "åŸºè¯ºè¯­çš„ç«æ€ä¹ˆè¯´ï¼Ÿ"
    keyword = extract_keywords(query, api_key)
    results = semantic_search(keyword, df, index, model)
    final = process_results(df, results)

    print("\nğŸ” æ£€ç´¢ç»“æœï¼š")
    for k, v in final.items():
        print(f"\nğŸ“ æ–‡æœ¬ï¼š{k}")
        print(f"ğŸ–¼ï¸ å›¾ç‰‡ï¼š{v['images']}")
        print(f"ğŸ”Š éŸ³é¢‘ï¼š{v['audios']}")
